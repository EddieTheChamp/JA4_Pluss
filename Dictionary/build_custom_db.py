import json
import argparse
import pandas as pd
from sklearn.model_selection import train_test_split

def build_custom_dataset(dataset_path: str, output_db_path: str):
    """
    Reads the full dataset, performs an 80/20 train/test split.
    Then, using strictly the Train split, removes duplicates and exports a db JSON
    file structured perfectly for the dictionary model to read.
    """
    print(f"Loading dataset from {dataset_path}...")
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # 1. Parse JSON into Pandas DataFrame
    df = pd.DataFrame(data)
    
    # Needs to have a target application
    df = df.dropna(subset=["application"])

    # 2. Train-Test Split (80% Training, 20% Testing) 
    # Must explicitly match the random_state across all scripts so the splits align purely in memory
    y = df["application"]
    try:
        train_df, _ = train_test_split(df, test_size=0.2, random_state=42, stratify=y)
    except ValueError:
        print("Warning: Couldn't stratify split due to rare classes. Falling back to unstratified split.")
        train_df, _ = train_test_split(df, test_size=0.2, random_state=42)

    print(f"Initial Training Samples: {len(train_df)}")

    # 3. Drop duplicates ONLY from the training dataset
    # We drop identical combinations of fingerprints AND application
    subset_cols = ["application", "ja4_fingerprint", "ja4s_fingerprint", "ja4ts_fingerprint", "ja4t_fingerprint"]
    existing_subset_cols = [col for col in subset_cols if col in train_df.columns]
    
    unique_train_df = train_df.drop_duplicates(subset=existing_subset_cols).reset_index(drop=True)
    print(f"Removed duplicates from the training set. {len(unique_train_df)} unique records remain for the DB.")

    # 4. Export the Database
    # We must format it like the official ja4+_db.json so the standard FoxIO model code can read it directly
    db_payload = []
    for _, row in unique_train_df.iterrows():
        # Using .get safely on the DataFrame series
        db_payload.append({
            "ja4_fingerprint": row.get("ja4_fingerprint"),
            "ja4s_fingerprint": row.get("ja4s_fingerprint"),
            "ja4ts_fingerprint": row.get("ja4ts_fingerprint"),
            "ja4t_fingerprint": row.get("ja4t_fingerprint"),
            "application": row.get("application"),
            "device": row.get("device"),
            "library": row.get("library"),
            "user_agent_string": row.get("user_agent_string"),
            "os": row.get("os"),
            "notes": "Egenlagd Autogenerated"
        })

    with open(output_db_path, 'w', encoding='utf-8') as f:
        json.dump(db_payload, f, indent=4)
        
    print(f"Successfully exported custom database to {output_db_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract unique training samples to build the Egenlagd DB.")
    parser.add_argument("--dataset_file", required=True, help="Path to the main JSON dataset file")
    parser.add_argument("--output_db", default="egenlagd_db.json", help="Path to save the generated DB")
    
    args = parser.parse_args()
    build_custom_dataset(args.dataset_file, args.output_db)
